{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype as is_num\n",
    "from river import tree\n",
    "\n",
    "chunk_size = 50000\n",
    "\n",
    "# Storage for training data\n",
    "all_X_train = []\n",
    "all_y_train = []\n",
    "total_X_test = pd.DataFrame()\n",
    "total_y_test = pd.Series(dtype='float64')\n",
    "\n",
    "# Columns\n",
    "all_columns = ['ID', 'vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
    "       'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag',\n",
    "       'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra',\n",
    "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
    "       'total_amount', 'congestion_surcharge', 'airport_fee', 'duration']\n",
    "\n",
    "columns_in_eval = ['ID', 'vendorid', 'tpep_pickup_datetime', 'passenger_count',\n",
    "       'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid',\n",
    "       'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax',\n",
    "       'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
    "       'congestion_surcharge', 'airport_fee']\n",
    "\n",
    "columns_not_in_eval = list(set(all_columns) - set(columns_in_eval))\n",
    "\n",
    "unneeded_columns = [ \n",
    "        'airport_fee', 'payment_type', 'congestion_surcharge',\n",
    "        'passenger_count', 'vendorid', 'improvement_surcharge', 'tolls_amount',\n",
    "        'extra', 'tip_amount', 'ratecodeid', 'store_and_fwd_flag',\n",
    "    ]\n",
    "\n",
    "needed_columns = list(set(all_columns) - set(unneeded_columns))\n",
    "\n",
    "# Read data in chunks\n",
    "chunks = pd.read_csv(\"training_dataset.csv\", chunksize=chunk_size, usecols=needed_columns)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, warm_start=True),\n",
    "    \"SGDRegressor\": SGDRegressor(alpha=0.0001, eta0=0.0001, learning_rate=\"adaptive\", warm_start=True),\n",
    "    \"HoeffdingTreeRegressor\": tree.HoeffdingTreeRegressor(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0...\n",
      "Processing chunk 1...\n",
      "Processing chunk 2...\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for df in chunks:\n",
    "    if counter == 3:\n",
    "        break\n",
    "    print(f\"Processing chunk {counter}...\")\n",
    "    counter += 1\n",
    "    \n",
    "    # Drop ID column\n",
    "    df.drop(columns=['ID'], inplace=True)\n",
    "    \n",
    "    # Handle missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Convert datetime columns\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['tpep_pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "    df.drop(columns=['tpep_pickup_datetime'], inplace=True)\n",
    "    df.drop(columns=['tpep_dropoff_datetime'], inplace=True)\n",
    "\n",
    "    \n",
    "    # for col in df.columns:\n",
    "    #     assert is_num(df[col]), f\"The '{col}' column contained categorical values\"\n",
    "    \n",
    "    \n",
    "    # # Outlier filtering\n",
    "    # df = df[(df['duration'] < 2880) & (df['duration'] > 30)]\n",
    "    # df = df[(df['trip_distance'] < 300) & (df['trip_distance'] > 0.25)]\n",
    "    # df = df[(df['fare_amount'] < 300) & (df['fare_amount'] > 0)]\n",
    "    # # df = df.applymap(lambda x: x if x > 0 else None).dropna()\n",
    "    # df = df[df.ge(0.01).all(1)]\n",
    "\n",
    "\n",
    "    # for col in df.columns:\n",
    "    #     assert (df[col] > 0).all(), f\"The '{col}' column contained negative values\"\n",
    "\n",
    "\n",
    "    # Split features and target\n",
    "    X_data = df.drop(columns = 'duration')\n",
    "    y_data = df['duration']\n",
    "\n",
    "\n",
    "    # Train-tt\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "\n",
    "    # Standardize features\n",
    "    #X_scaler = RobustScaler(quantile_range=(1.0, 99.0))\n",
    "    X_scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(X_scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_test = pd.DataFrame(X_scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "    #y_Scaler = RobustScaler(quantile_range=(1.0, 99.0))\n",
    "    y_Scaler = StandardScaler()\n",
    "    y_train = y_Scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_test = pd.Series(y_Scaler.transform(y_test.values.reshape(-1, 1)).flatten(), index=X_test.index)\n",
    "\n",
    "\n",
    "    # Accumulate test data\n",
    "    total_X_test = pd.concat([total_X_test, X_test])\n",
    "    total_y_test = pd.concat([total_y_test, y_test])\n",
    "    \n",
    "    # Train the model chunk by chunk    \n",
    "    for model_name in models.keys():\n",
    "            if model_name == \"RandomForestRegressor\":\n",
    "                if counter == 0:\n",
    "                    models[model_name].fit(X_train, y_train)  # First chunk: normal fit\n",
    "                else:\n",
    "                    models[model_name].n_estimators += 10  # Increase trees\n",
    "                    models[model_name].fit(X_train, y_train)  # Fit on new chunk\n",
    "            elif model_name == \"SGDRegressor\":\n",
    "                models[model_name].partial_fit(X_train, y_train)\n",
    "            elif model_name == \"HoeffdingTreeRegressor\":\n",
    "                for x, y in zip(X_train.to_dict(orient=\"records\"), y_train):\n",
    "                    models[model_name].learn_one(x, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE RandomForestRegressor: 1.0914364651118633\n",
      "Score RandomForestRegressor: 0.03183755159110302\n",
      "MSE SGDRegressor: 967.2038430687493\n",
      "Score SGDRegressor: -856.9614762275334\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HoeffdingTreeRegressor' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:     \n\u001b[1;32m---> 14\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(total_X_test)\n\u001b[0;32m     15\u001b[0m     mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_true\u001b[38;5;241m=\u001b[39mtotal_y_test, y_pred\u001b[38;5;241m=\u001b[39my_pred)\n\u001b[0;32m     17\u001b[0m     score \u001b[38;5;241m=\u001b[39m r2_score(y_true\u001b[38;5;241m=\u001b[39mtotal_y_test, y_pred\u001b[38;5;241m=\u001b[39my_pred)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HoeffdingTreeRegressor' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "for model_name in models.keys():\n",
    "    if model_name == \"SGDRegressor\":\n",
    "        y_pred = y_Scaler.inverse_transform(models[\"SGDRegressor\"].predict(X_test).reshape(-1,1)).flatten()\n",
    "        mse = mean_squared_error(y_Scaler.inverse_transform(y_test.values.reshape(-1,1)).flatten(), y_pred)\n",
    "        y_pred = models[model_name].predict(total_X_test)\n",
    "        mse = mean_squared_error(y_true=total_y_test, y_pred=y_pred)\n",
    "        \n",
    "        score = r2_score(y_true=total_y_test, y_pred=y_pred)\n",
    "\n",
    "        print(f\"MSE {model_name}: {mse}\")\n",
    "        print(f\"Score {model_name}: {score}\")\n",
    "    else:     \n",
    "        y_pred = models[model_name].predict(total_X_test)\n",
    "        mse = mean_squared_error(y_true=total_y_test, y_pred=y_pred)\n",
    "        \n",
    "        score = r2_score(y_true=total_y_test, y_pred=y_pred)\n",
    "\n",
    "        print(f\"MSE {model_name}: {mse}\")\n",
    "        print(f\"Score {model_name}: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df_eval = pd.read_csv(\"eval.csv\")\n",
    "\n",
    "# Store IDs for final output\n",
    "eval_ids = df_eval[\"ID\"]\n",
    "\n",
    "# Drop ID column\n",
    "df_eval.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "# Convert datetime column\n",
    "df_eval['tpep_pickup_datetime'] = pd.to_datetime(df_eval['tpep_pickup_datetime'])\n",
    "df_eval['tpep_pickup_hour'] = df_eval['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "cols_to_drop = unneeded_columns + ['tpep_pickup_datetime']\n",
    "# Drop only existing columns\n",
    "df_eval.drop(columns=[col for col in cols_to_drop if col in df_eval.columns], inplace=True)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_eval = models[\"RandomForestRegressor\"].predict(df_eval)\n",
    "\n",
    "# Save output\n",
    "df_out = pd.DataFrame({\"ID\": eval_ids, \"duration\": y_pred_eval}).set_index(\"ID\")\n",
    "df_out.to_csv(\"submission.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
