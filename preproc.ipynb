{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Processing chunk 7...\n",
      "Processing chunk 8...\n",
      "Processing chunk 9...\n",
      "Processing chunk 10...\n",
      "Processing chunk 11...\n",
      "Processing chunk 12...\n",
      "Processing chunk 13...\n",
      "Processing chunk 14...\n",
      "Processing chunk 15...\n",
      "Processing chunk 16...\n",
      "Processing chunk 17...\n",
      "Processing chunk 18...\n",
      "Processing chunk 19...\n",
      "Processing chunk 20...\n",
      "Processing chunk 21...\n",
      "Processing chunk 22...\n",
      "Processing chunk 23...\n",
      "Processing chunk 24...\n",
      "Processing chunk 25...\n",
      "Processing chunk 26...\n",
      "Processing chunk 27...\n",
      "Processing chunk 28...\n",
      "Processing chunk 29...\n",
      "Processing chunk 30...\n",
      "Processing chunk 31...\n",
      "Processing chunk 32...\n",
      "Processing chunk 33...\n",
      "Processing chunk 34...\n",
      "Processing chunk 35...\n",
      "Processing chunk 36...\n",
      "Processing chunk 37...\n",
      "Processing chunk 38...\n",
      "Processing chunk 39...\n",
      "Processing chunk 40...\n",
      "Processing chunk 41...\n",
      "Processing chunk 42...\n",
      "Processing chunk 43...\n",
      "Processing chunk 44...\n",
      "Processing chunk 45...\n",
      "Processing chunk 46...\n",
      "Processing chunk 47...\n",
      "Processing chunk 48...\n",
      "Processing chunk 49...\n",
      "Processing chunk 50...\n",
      "Processing chunk 51...\n",
      "Processing chunk 52...\n",
      "Processing chunk 53...\n",
      "Processing chunk 54...\n",
      "Processing chunk 55...\n",
      "Processing chunk 56...\n",
      "Processing chunk 57...\n",
      "Processing chunk 58...\n",
      "Processing chunk 59...\n",
      "Processing chunk 60...\n",
      "Processing chunk 61...\n",
      "Processing chunk 62...\n",
      "Processing chunk 63...\n",
      "Processing chunk 64...\n",
      "Processing chunk 65...\n",
      "Processing chunk 66...\n",
      "Processing chunk 67...\n",
      "Processing chunk 68...\n",
      "Processing chunk 69...\n",
      "Processing chunk 70...\n",
      "Processing chunk 71...\n",
      "Processing chunk 72...\n",
      "Processing chunk 73...\n",
      "Processing chunk 74...\n",
      "Processing chunk 75...\n",
      "Processing chunk 76...\n",
      "Processing chunk 77...\n",
      "Processing chunk 78...\n",
      "Processing chunk 79...\n",
      "Processing chunk 80...\n",
      "Processing chunk 81...\n",
      "Processing chunk 82...\n",
      "Processing chunk 83...\n",
      "Processing chunk 84...\n",
      "Processing chunk 85...\n",
      "Processing chunk 86...\n",
      "Processing chunk 87...\n",
      "Processing chunk 88...\n",
      "Processing chunk 89...\n",
      "Processing chunk 90...\n",
      "Processing chunk 91...\n",
      "Processing chunk 92...\n",
      "Processing chunk 93...\n",
      "Processing chunk 94...\n",
      "Processing chunk 95...\n",
      "Processing chunk 96...\n",
      "Processing chunk 97...\n",
      "Processing chunk 98...\n",
      "Processing chunk 99...\n",
      "Processing chunk 100...\n",
      "Processing chunk 101...\n",
      "Processing chunk 102...\n",
      "Processing chunk 103...\n",
      "Processing chunk 104...\n",
      "Processing chunk 105...\n",
      "Processing chunk 106...\n",
      "Processing chunk 107...\n",
      "Processing chunk 108...\n",
      "Processing chunk 109...\n",
      "Processing chunk 110...\n",
      "Processing chunk 111...\n",
      "Processing chunk 112...\n",
      "Processing chunk 113...\n",
      "Processing chunk 114...\n",
      "Processing chunk 115...\n",
      "Processing chunk 116...\n",
      "Processing chunk 117...\n",
      "Processing chunk 118...\n",
      "Processing chunk 119...\n",
      "Processing chunk 120...\n",
      "Processing chunk 121...\n",
      "Processing chunk 122...\n",
      "Processing chunk 123...\n",
      "Processing chunk 124...\n",
      "Processing chunk 125...\n",
      "Processing chunk 126...\n",
      "Processing chunk 127...\n",
      "Processing chunk 128...\n",
      "Processing chunk 129...\n",
      "Processing chunk 130...\n",
      "Processing chunk 131...\n",
      "Processing chunk 132...\n",
      "Processing chunk 133...\n",
      "Processing chunk 134...\n",
      "Processing chunk 135...\n",
      "Processing chunk 136...\n",
      "Processing chunk 137...\n",
      "Processing chunk 138...\n",
      "Processing chunk 139...\n",
      "Processing chunk 140...\n",
      "Processing chunk 141...\n",
      "Processing chunk 142...\n",
      "Processing chunk 143...\n",
      "Processing chunk 144...\n",
      "Processing chunk 145...\n",
      "Processing chunk 146...\n",
      "Processing chunk 147...\n",
      "Processing chunk 148...\n",
      "Processing chunk 149...\n",
      "Processing chunk 150...\n",
      "Processing chunk 151...\n",
      "Processing chunk 152...\n",
      "Processing chunk 153...\n",
      "Processing chunk 154...\n",
      "Processing chunk 155...\n",
      "Processing chunk 156...\n",
      "Processing chunk 157...\n",
      "Processing chunk 158...\n",
      "Processing chunk 159...\n",
      "Processing chunk 160...\n",
      "Processing chunk 161...\n",
      "Processing chunk 162...\n",
      "Processing chunk 163...\n",
      "Processing chunk 164...\n",
      "Processing chunk 165...\n",
      "Processing chunk 166...\n",
      "Processing chunk 167...\n",
      "Processing chunk 168...\n",
      "Processing chunk 169...\n",
      "Processing chunk 170...\n",
      "Processing chunk 171...\n",
      "Processing chunk 172...\n",
      "Processing chunk 173...\n",
      "Processing chunk 174...\n",
      "Processing chunk 175...\n",
      "Processing chunk 176...\n",
      "Processing chunk 177...\n",
      "Processing chunk 178...\n",
      "Processing chunk 179...\n",
      "Processing chunk 180...\n",
      "Processing chunk 181...\n",
      "Processing chunk 182...\n",
      "Processing chunk 183...\n",
      "Processing chunk 184...\n",
      "Processing chunk 185...\n",
      "Processing chunk 186...\n",
      "Processing chunk 187...\n",
      "Processing chunk 188...\n",
      "Processing chunk 189...\n",
      "Processing chunk 190...\n",
      "Processing chunk 191...\n",
      "Processing chunk 192...\n",
      "Processing chunk 193...\n",
      "Processing chunk 194...\n",
      "Processing chunk 195...\n",
      "Processing chunk 196...\n",
      "Processing chunk 197...\n",
      "Processing chunk 198...\n",
      "Processing chunk 199...\n",
      "Processing chunk 200...\n",
      "Processing chunk 201...\n",
      "Processing chunk 202...\n",
      "Processing chunk 203...\n",
      "Processing chunk 204...\n",
      "Processing chunk 205...\n",
      "Processing chunk 206...\n",
      "Processing chunk 207...\n",
      "Processing chunk 208...\n",
      "Processing chunk 209...\n",
      "Processing chunk 210...\n",
      "Processing chunk 211...\n",
      "Processing chunk 212...\n",
      "Processing chunk 213...\n",
      "Processing chunk 214...\n",
      "Processing chunk 215...\n",
      "Processing chunk 216...\n",
      "Processing chunk 217...\n",
      "Processing chunk 218...\n",
      "Processing chunk 219...\n",
      "Processing chunk 220...\n",
      "Processing chunk 221...\n",
      "Processing chunk 222...\n",
      "Processing chunk 223...\n",
      "Processing chunk 224...\n",
      "Processing chunk 225...\n",
      "Processing chunk 226...\n",
      "Processing chunk 227...\n",
      "Processing chunk 228...\n",
      "Processing chunk 229...\n",
      "Processing chunk 230...\n",
      "Processing chunk 231...\n",
      "Processing chunk 232...\n",
      "Processing chunk 233...\n",
      "Processing chunk 234...\n",
      "Processing chunk 235...\n",
      "Processing chunk 236...\n",
      "Processing chunk 237...\n",
      "Processing chunk 238...\n",
      "Processing chunk 239...\n",
      "Processing chunk 240...\n",
      "Processing chunk 241...\n",
      "Processing chunk 242...\n",
      "Processing chunk 243...\n",
      "Processing chunk 244...\n",
      "Processing chunk 245...\n",
      "Processing chunk 246...\n",
      "Processing chunk 247...\n",
      "Processing chunk 248...\n",
      "Processing chunk 249...\n",
      "Processing chunk 250...\n",
      "Processing chunk 251...\n",
      "Processing chunk 252...\n",
      "Processing chunk 253...\n",
      "Processing chunk 254...\n",
      "Processing chunk 255...\n",
      "Processing chunk 256...\n",
      "Processing chunk 257...\n",
      "Processing chunk 258...\n",
      "Processing chunk 259...\n",
      "Processing chunk 260...\n",
      "Processing chunk 261...\n",
      "Processing chunk 262...\n",
      "Processing chunk 263...\n",
      "Processing chunk 264...\n",
      "Processing chunk 265...\n",
      "Processing chunk 266...\n",
      "Processing chunk 267...\n",
      "Processing chunk 268...\n",
      "Processing chunk 269...\n",
      "Processing chunk 270...\n",
      "Processing chunk 271...\n",
      "Processing chunk 272...\n",
      "Processing chunk 273...\n",
      "Processing chunk 274...\n",
      "Processing chunk 275...\n",
      "Processing chunk 276...\n",
      "Processing chunk 277...\n",
      "Processing chunk 278...\n",
      "Processing chunk 279...\n",
      "Processing chunk 280...\n",
      "Processing chunk 281...\n",
      "Processing chunk 282...\n",
      "Processing chunk 283...\n",
      "Processing chunk 284...\n",
      "Processing chunk 285...\n",
      "Processing chunk 286...\n",
      "Processing chunk 287...\n",
      "Processing chunk 288...\n",
      "Processing chunk 289...\n",
      "Processing chunk 290...\n",
      "Processing chunk 291...\n",
      "Processing chunk 292...\n",
      "Processing chunk 293...\n",
      "Processing chunk 294...\n",
      "Processing chunk 295...\n",
      "Processing chunk 296...\n",
      "Processing chunk 297...\n",
      "Processing chunk 298...\n",
      "Processing chunk 299...\n",
      "Processing chunk 300...\n",
      "Processing chunk 301...\n",
      "Processing chunk 302...\n",
      "Processing chunk 303...\n",
      "Processing chunk 304...\n",
      "Processing chunk 305...\n",
      "Processing chunk 306...\n",
      "Processing chunk 307...\n",
      "Processing chunk 308...\n",
      "Processing chunk 309...\n",
      "Processing chunk 310...\n",
      "Processing chunk 311...\n",
      "Processing chunk 312...\n",
      "Processing chunk 313...\n",
      "Processing chunk 314...\n",
      "Processing chunk 315...\n",
      "Processing chunk 316...\n",
      "Processing chunk 317...\n",
      "Processing chunk 318...\n",
      "Processing chunk 319...\n",
      "Processing chunk 320...\n",
      "Processing chunk 321...\n",
      "Processing chunk 322...\n",
      "Processing chunk 323...\n",
      "Processing chunk 324...\n",
      "Processing chunk 325...\n",
      "Processing chunk 326...\n",
      "Processing chunk 327...\n",
      "Processing chunk 328...\n",
      "Processing chunk 329...\n",
      "Processing chunk 330...\n",
      "Processing chunk 331...\n",
      "Processing chunk 332...\n",
      "Processing chunk 333...\n",
      "Processing chunk 334...\n",
      "Processing chunk 335...\n",
      "Processing chunk 336...\n",
      "Processing chunk 337...\n",
      "Processing chunk 338...\n",
      "Processing chunk 339...\n",
      "Processing chunk 340...\n",
      "Processing chunk 341...\n",
      "Processing chunk 342...\n",
      "Processing chunk 343...\n",
      "Processing chunk 344...\n",
      "Processing chunk 345...\n",
      "Processing chunk 346...\n",
      "Processing chunk 347...\n",
      "Processing chunk 348...\n",
      "Processing chunk 349...\n",
      "Processing chunk 350...\n",
      "Processing chunk 351...\n",
      "Processing chunk 352...\n",
      "Processing chunk 353...\n",
      "Processing chunk 354...\n",
      "Processing chunk 355...\n",
      "Processing chunk 356...\n",
      "Processing chunk 357...\n",
      "Processing chunk 358...\n",
      "Processing chunk 359...\n",
      "Processing chunk 360...\n",
      "Processing chunk 361...\n",
      "Processing chunk 362...\n",
      "Processing chunk 363...\n",
      "Processing chunk 364...\n",
      "Processing chunk 365...\n",
      "Processing chunk 366...\n",
      "Processing chunk 367...\n",
      "Processing chunk 368...\n",
      "Processing chunk 369...\n",
      "Processing chunk 370...\n",
      "Processing chunk 371...\n",
      "Processing chunk 372...\n",
      "Processing chunk 373...\n",
      "Processing chunk 374...\n",
      "Processing chunk 375...\n",
      "Processing chunk 376...\n",
      "Processing chunk 377...\n",
      "Processing chunk 378...\n",
      "Processing chunk 379...\n",
      "Processing chunk 380...\n",
      "Processing chunk 381...\n",
      "Processing chunk 382...\n",
      "Processing chunk 383...\n",
      "Processing chunk 384...\n",
      "Processing chunk 385...\n",
      "Processing chunk 386...\n",
      "Processing chunk 387...\n",
      "Processing chunk 388...\n",
      "Processing chunk 389...\n",
      "Processing chunk 390...\n",
      "Processing chunk 391...\n",
      "Processing chunk 392...\n",
      "Processing chunk 393...\n",
      "Processing chunk 394...\n",
      "Processing chunk 395...\n",
      "Processing chunk 396...\n",
      "Processing chunk 397...\n",
      "Processing chunk 398...\n",
      "Processing chunk 399...\n",
      "Processing chunk 400...\n",
      "Processing chunk 401...\n",
      "Processing chunk 402...\n",
      "Processing chunk 403...\n",
      "Processing chunk 404...\n",
      "Processing chunk 405...\n",
      "Processing chunk 406...\n",
      "Processing chunk 407...\n",
      "Processing chunk 408...\n",
      "Processing chunk 409...\n",
      "Processing chunk 410...\n",
      "Processing chunk 411...\n",
      "Processing chunk 412...\n",
      "Processing chunk 413...\n",
      "Processing chunk 414...\n",
      "Processing chunk 415...\n",
      "Processing chunk 416...\n",
      "Processing chunk 417...\n",
      "Processing chunk 418...\n",
      "Processing chunk 419...\n",
      "Processing chunk 420...\n",
      "Processing chunk 421...\n",
      "Processing chunk 422...\n",
      "Processing chunk 423...\n",
      "Processing chunk 424...\n",
      "Processing chunk 425...\n",
      "Processing chunk 426...\n",
      "Processing chunk 427...\n",
      "Processing chunk 428...\n",
      "Processing chunk 429...\n",
      "Processing chunk 430...\n",
      "Processing chunk 431...\n",
      "Processing chunk 432...\n",
      "Processing chunk 433...\n",
      "Processing chunk 434...\n",
      "Processing chunk 435...\n",
      "Processing chunk 436...\n",
      "Processing chunk 437...\n",
      "Processing chunk 438...\n",
      "Processing chunk 439...\n",
      "Processing chunk 440...\n",
      "Processing chunk 441...\n",
      "Processing chunk 442...\n",
      "Processing chunk 443...\n",
      "Processing chunk 444...\n",
      "Processing chunk 445...\n",
      "Processing chunk 446...\n",
      "Processing chunk 447...\n",
      "Processing chunk 448...\n",
      "Processing chunk 449...\n",
      "Processing chunk 450...\n",
      "Processing chunk 451...\n",
      "Processing chunk 452...\n",
      "Processing chunk 453...\n",
      "Processing chunk 454...\n",
      "Processing chunk 455...\n",
      "Processing chunk 456...\n",
      "Processing chunk 457...\n",
      "Processing chunk 458...\n",
      "Processing chunk 459...\n",
      "Processing chunk 460...\n",
      "Processing chunk 461...\n",
      "Processing chunk 462...\n",
      "Processing chunk 463...\n",
      "Processing chunk 464...\n",
      "Processing chunk 465...\n",
      "Processing chunk 466...\n",
      "Processing chunk 467...\n",
      "Processing chunk 468...\n",
      "Processing chunk 469...\n",
      "Processing chunk 470...\n",
      "Processing chunk 471...\n",
      "Processing chunk 472...\n",
      "Processing chunk 473...\n",
      "Processing chunk 474...\n",
      "Processing chunk 475...\n",
      "Processing chunk 476...\n",
      "Processing chunk 477...\n",
      "Processing chunk 478...\n",
      "Processing chunk 479...\n",
      "Processing chunk 480...\n",
      "Processing chunk 481...\n",
      "Processing chunk 482...\n",
      "Processing chunk 483...\n",
      "Processing chunk 484...\n",
      "Processing chunk 485...\n",
      "Processing chunk 486...\n",
      "Processing chunk 487...\n",
      "Processing chunk 488...\n",
      "Processing chunk 489...\n",
      "Processing chunk 490...\n",
      "Processing chunk 491...\n",
      "Processing chunk 492...\n",
      "Processing chunk 493...\n",
      "Processing chunk 494...\n",
      "Processing chunk 495...\n",
      "Processing chunk 496...\n",
      "Processing chunk 497...\n",
      "Processing chunk 498...\n",
      "Processing chunk 499...\n",
      "Processing chunk 500...\n",
      "Processing chunk 501...\n",
      "Processing chunk 502...\n",
      "Processing chunk 503...\n",
      "Processing chunk 504...\n",
      "Processing chunk 505...\n",
      "Processing chunk 506...\n",
      "Processing chunk 507...\n",
      "Processing chunk 508...\n",
      "Processing chunk 509...\n",
      "Processing chunk 510...\n",
      "Processing chunk 511...\n",
      "Processing chunk 512...\n",
      "Processing chunk 513...\n",
      "Processing chunk 514...\n",
      "Processing chunk 515...\n",
      "Processing chunk 516...\n",
      "Processing chunk 517...\n",
      "Processing chunk 518...\n",
      "Processing chunk 519...\n",
      "Processing chunk 520...\n",
      "Processing chunk 521...\n",
      "Processing chunk 522...\n",
      "Processing chunk 523...\n",
      "Processing chunk 524...\n",
      "Processing chunk 525...\n",
      "Processing chunk 526...\n",
      "Processing chunk 527...\n",
      "Processing chunk 528...\n",
      "Processing chunk 529...\n",
      "Processing chunk 530...\n",
      "Processing chunk 531...\n",
      "Processing chunk 532...\n",
      "Processing chunk 533...\n",
      "Processing chunk 534...\n",
      "Processing chunk 535...\n",
      "Processing chunk 536...\n",
      "Processing chunk 537...\n",
      "Processing chunk 538...\n",
      "Processing chunk 539...\n",
      "Processing chunk 540...\n",
      "Processing chunk 541...\n",
      "Processing chunk 542...\n",
      "Processing chunk 543...\n",
      "Processing chunk 544...\n",
      "Processing chunk 545...\n",
      "Processing chunk 546...\n",
      "Processing chunk 547...\n",
      "Processing chunk 548...\n",
      "Processing chunk 549...\n",
      "Processing chunk 550...\n",
      "Processing chunk 551...\n",
      "Processing chunk 552...\n",
      "Processing chunk 553...\n",
      "Processing chunk 554...\n",
      "Processing chunk 555...\n",
      "Processing chunk 556...\n",
      "Processing chunk 557...\n",
      "Processing chunk 558...\n",
      "Processing chunk 559...\n",
      "Processing chunk 560...\n",
      "Processing chunk 561...\n",
      "Processing chunk 562...\n",
      "Processing chunk 563...\n",
      "Processing chunk 564...\n",
      "Processing chunk 565...\n",
      "Processing chunk 566...\n",
      "Processing chunk 567...\n",
      "Processing chunk 568...\n",
      "Processing chunk 569...\n",
      "Processing chunk 570...\n",
      "Processing chunk 571...\n",
      "Processing chunk 572...\n",
      "Processing chunk 573...\n",
      "Processing chunk 574...\n",
      "Processing chunk 575...\n",
      "Processing chunk 576...\n",
      "Processing chunk 577...\n",
      "Processing chunk 578...\n",
      "Processing chunk 579...\n",
      "Processing chunk 580...\n",
      "Processing chunk 581...\n",
      "Processing chunk 582...\n",
      "Processing chunk 583...\n",
      "Processing chunk 584...\n",
      "Processing chunk 585...\n",
      "Processing chunk 586...\n",
      "Processing chunk 587...\n",
      "Processing chunk 588...\n",
      "Processing chunk 589...\n",
      "Processing chunk 590...\n",
      "Processing chunk 591...\n",
      "Processing chunk 592...\n",
      "Processing chunk 593...\n",
      "Processing chunk 594...\n",
      "Processing chunk 595...\n",
      "Processing chunk 596...\n",
      "Processing chunk 597...\n",
      "Processing chunk 598...\n",
      "Processing chunk 599...\n",
      "Processing chunk 600...\n",
      "Processing chunk 601...\n",
      "Processing chunk 602...\n",
      "Processing chunk 603...\n",
      "Processing chunk 604...\n",
      "Processing chunk 605...\n",
      "Processing chunk 606...\n",
      "Processing chunk 607...\n",
      "Processing chunk 608...\n",
      "Processing chunk 609...\n",
      "Processing chunk 610...\n",
      "Processing chunk 611...\n",
      "Processing chunk 612...\n",
      "Processing chunk 613...\n",
      "Processing chunk 614...\n",
      "Processing chunk 615...\n",
      "Processing chunk 616...\n",
      "Processing chunk 617...\n",
      "Processing chunk 618...\n",
      "Processing chunk 619...\n",
      "Processing chunk 620...\n",
      "Processing chunk 621...\n",
      "Processing chunk 622...\n",
      "Processing chunk 623...\n",
      "Processing chunk 624...\n",
      "Processing chunk 625...\n",
      "Processing chunk 626...\n",
      "Processing chunk 627...\n",
      "Processing chunk 628...\n",
      "Processing chunk 629...\n",
      "Processing chunk 630...\n",
      "Processing chunk 631...\n",
      "Processing chunk 632...\n",
      "Processing chunk 633...\n",
      "Processing chunk 634...\n",
      "Processing chunk 635...\n",
      "Processing chunk 636...\n",
      "Processing chunk 637...\n",
      "Processing chunk 638...\n",
      "Processing chunk 639...\n",
      "Processing chunk 640...\n",
      "Processing chunk 641...\n",
      "Processing chunk 642...\n",
      "Processing chunk 643...\n",
      "Processing chunk 644...\n",
      "Processing chunk 645...\n",
      "Processing chunk 646...\n",
      "Processing chunk 647...\n",
      "Processing chunk 648...\n",
      "Processing chunk 649...\n",
      "Processing chunk 650...\n",
      "Processing chunk 651...\n",
      "Processing chunk 652...\n",
      "Processing chunk 653...\n",
      "Processing chunk 654...\n",
      "Processing chunk 655...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 2147483648 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\joblib\\_utils.py\", line 72, in __call__\n    return self.func(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 189, in _parallel_build_trees\n    tree._fit(\n  File \"c:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 472, in _fit\n    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n  File \"_tree.pyx\", line 153, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"_tree.pyx\", line 268, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"_tree.pyx\", line 923, in sklearn.tree._tree.Tree._add_node\n  File \"_tree.pyx\", line 891, in sklearn.tree._tree.Tree._resize_c\n  File \"_utils.pyx\", line 29, in sklearn.tree._utils.safe_realloc\nMemoryError: could not allocate 2147483648 bytes\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1316], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m reg \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m---> 91\u001b[0m     reg[model_name] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_y_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Evaluate models\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32mc:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    479\u001b[0m ]\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jonna\\Dev\\IntroMask\\.venv\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: could not allocate 2147483648 bytes"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "chunk_size = 50000\n",
    "\n",
    "# Initialize models\n",
    "model = {\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Storage for training data\n",
    "all_X_train = []\n",
    "all_y_train = []\n",
    "total_X_test = pd.DataFrame()\n",
    "total_y_test = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for df in pd.read_csv(\"training_dataset.csv\", chunksize=chunk_size):\n",
    "    counter += 1\n",
    "    print(f\"Processing chunk {counter}...\")\n",
    "\n",
    "    # Handle missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Rename columns\n",
    "    column_names = [\"ID\", \"vendorid\",\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\"passenger_count\", \"trip_distance\",\n",
    "                    \"ratecodeid\",\"store_and_fwd_flag\",\"pulocationid\",\"dolocationid\",\"payment_type\",\"fare_amount\",\n",
    "                    \"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"improvement_surcharge\",\"total_amount\",\n",
    "                    \"congestion_surcharge\",\"airport_fee\",\"duration\"]                 \n",
    "    df.columns = column_names\n",
    "    df.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "    # Convert datetime columns\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "\n",
    "    df['tpep_pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "    df.drop(columns=['tpep_pickup_datetime'], inplace=True)\n",
    "\n",
    "    # Outlier filtering\n",
    "    df = df[(df['duration'] < 2880) & (df['duration'] > 30)]\n",
    "    df = df[(df['trip_distance'] < 300) & (df['trip_distance'] > 0.25)]\n",
    "\n",
    "    # Drop unneeded columns\n",
    "    df.drop(columns=[\n",
    "        'airport_fee', 'payment_type', 'congestion_surcharge',\n",
    "        'passenger_count', 'vendorid', 'improvement_surcharge', 'tolls_amount',\n",
    "        'extra', 'tip_amount'\n",
    "    ], inplace=True)\n",
    "\n",
    "    # Split features and target\n",
    "    X_data = df.drop(columns=['duration', \"tpep_dropoff_datetime\", 'ratecodeid', 'store_and_fwd_flag'])\n",
    "    y_data = df['duration']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Accumulate training data\n",
    "    all_X_train.append(X_train)\n",
    "    all_y_train.append(y_train)\n",
    "\n",
    "    # Accumulate test data\n",
    "    total_X_test = pd.concat([total_X_test, X_test])\n",
    "    total_y_test = pd.concat([total_y_test, y_test])\n",
    "\n",
    "\n",
    "# Combine all training data\n",
    "final_X_train = pd.concat(all_X_train)\n",
    "final_y_train = pd.concat(all_y_train)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "final_X_train = pd.DataFrame(scaler.fit_transform(final_X_train), columns=final_X_train.columns)\n",
    "total_X_test = pd.DataFrame(scaler.transform(total_X_test), columns=total_X_test.columns)\n",
    "\n",
    "# Standardize the target variable\n",
    "scaler_y = StandardScaler()\n",
    "final_y_train = scaler_y.fit_transform(final_y_train.values.reshape(-1, 1)).flatten()\n",
    "total_y_test = scaler_y.transform(total_y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Train models only once\n",
    "reg = {}\n",
    "for model_name in model.keys():\n",
    "    \n",
    "    reg[model_name] = model[model_name].fit(final_X_train, final_y_train)\n",
    "\n",
    "# Evaluate models\n",
    "for model_name in model.keys():\n",
    "    y_pred = reg[model_name].predict(total_X_test)\n",
    "    mse = mean_squared_error(y_true=total_y_test, y_pred=y_pred)\n",
    "    \n",
    "    score = r2_score(y_true=total_y_test, y_pred=y_pred)\n",
    "\n",
    "    print(f\"MSE {model_name}: {mse}\")\n",
    "    print(f\"Score {model_name}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       trip_distance   pulocationid   dolocationid    fare_amount  \\\n",
      "count  500000.000000  500000.000000  500000.000000  500000.000000   \n",
      "mean        5.765083     164.262598     163.553426      19.275166   \n",
      "std       630.518207      64.314410      69.545243      19.410251   \n",
      "min         0.000000       1.000000       1.000000   -1174.100000   \n",
      "25%         1.010000     132.000000     113.000000       9.300000   \n",
      "50%         1.760000     161.000000     162.000000      13.500000   \n",
      "75%         3.370000     233.000000     234.000000      22.600000   \n",
      "max    319111.840000     265.000000     265.000000    1349.800000   \n",
      "\n",
      "             mta_tax   total_amount  tpep_pickup_hour  \n",
      "count  500000.000000  500000.000000     500000.000000  \n",
      "mean        0.480351      27.853946         14.304504  \n",
      "std         0.127669      24.121979          5.847429  \n",
      "min        -0.500000    -963.880000          0.000000  \n",
      "25%         0.500000      15.750000         11.000000  \n",
      "50%         0.500000      21.000000         15.000000  \n",
      "75%         0.500000      30.590000         19.000000  \n",
      "max         3.000000    1374.370000         23.000000  \n"
     ]
    }
   ],
   "source": [
    "df_eval = pd.read_csv(\"eval.csv\")\n",
    "\n",
    "# df_eval = df_eval.dropna()\n",
    "\n",
    "# Rename columns\n",
    "column_names = [\"ID\", \"vendorid\",\"tpep_pickup_datetime\",\"passenger_count\", \"trip_distance\",\n",
    "                \"ratecodeid\",\"store_and_fwd_flag\",\"pulocationid\",\"dolocationid\",\"payment_type\",\"fare_amount\",\n",
    "                \"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"improvement_surcharge\",\"total_amount\",\n",
    "                \"congestion_surcharge\",\"airport_fee\"]                  \n",
    "\n",
    "df_eval.columns = column_names\n",
    "\n",
    "eval_ids = df_eval[\"ID\"]\n",
    "\n",
    "df_eval.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "# Convert datetime columns\n",
    "df_eval['tpep_pickup_datetime'] = pd.to_datetime(df_eval['tpep_pickup_datetime'])\n",
    "\n",
    "df_eval['tpep_pickup_hour'] = df_eval['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "df_eval.drop(columns=['tpep_pickup_datetime', 'store_and_fwd_flag', 'ratecodeid'], inplace=True)\n",
    "\n",
    "# Drop unneeded columns\n",
    "df_eval.drop(columns=[\n",
    "    'airport_fee', 'payment_type', 'congestion_surcharge',\n",
    "    'passenger_count', 'vendorid', 'improvement_surcharge', 'tolls_amount',\n",
    "    'extra', 'tip_amount'\n",
    "], inplace=True)\n",
    "\n",
    "print(df_eval.describe())\n",
    "\n",
    "y_pred_eval = reg[\"RandomForestRegressor\"].predict(df_eval)\n",
    "\n",
    "df_out = pd.DataFrame({\"ID\": eval_ids, \"duration\": y_pred_eval}).set_index(\"ID\")\n",
    "df_out.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
